---
title: "Structural Topic Modelling"
author: "Malte LÃ¼ken"
date: '2022-06-14'
output: html_document
params:
  fit: False
---

```{r setup, include=FALSE}

# Set params$fit to True to fit the structural topic models

knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r load-libraries}

library(rjson)
library(dplyr)
library(stringr)
library(tidyr)
library(ggplot2)
library(ggExtra)
library(ggrepel)
library(quanteda)
library(stm)
library(stminsights)

```

```{r load-docs}

# Function for excluding chapters
exclude_sections = function(doc, terms) {
  exclude = str_detect(tolower(names(doc)), pattern = paste(terms, collapse = '|'))
  
  return(doc[!exclude])
}

# Read json files with scraped text from pdfs
json_filenames = list.files(
  file.path('..', 'scrape-corpus', 'studies_on_water_scraped'),
  full.names = TRUE
)

docs = lapply(json_filenames, function(filename) {fromJSON(file = filename)})

# Strings for excluding chapters
terms = c(
  'preface', 'foreword', 'acknowledg', 'executive', 'summary', 'table',
  'figure', 'box', 'abbreviation', 'acronym', 'glossary', 'bibliography', 
  'note', 'meta', 'further reading', 'page',  'key messages', 'annex', 
  'refere', 'background materials', 'statistics'
)

# Exclude irrelevant chapters
docs_relevant = lapply(docs, exclude_sections, terms = terms)

# Create list of chapters included in analysis
included_chapters = str_remove_all(
  sapply(
    lapply(docs_relevant, names), paste, collapse = '\n\t'
  ),
  pattern = '\r'
)

write.table(included_chapters, file = 'included_chapters.txt', quote = FALSE)

```


```{r load-metadata}

# Read metadata
meta_data = read.csv('studies_on_water_metadata.csv', sep = ';', nrows = 55) %>%
  select(-Number) %>%
  arrange(title) %>%
  mutate(id = row_number(), .before = 1)

# Create corpus object from text and metadata
docs_corpus = corpus(
  sapply(docs_relevant, function(doc) paste(unlist(doc), collapse = ' ')),
  docnames = sapply(docs, function(doc) doc$meta$title),
  docvars = meta_data
)

summary(docs_corpus, n = 5)

```

```{r prep-dfm}

# Create tokenized corpus
docs_tokens = docs_corpus %>%
  tokens(
    remove_punct = TRUE,
    remove_symbols = TRUE,
    remove_numbers = TRUE,
    remove_url = TRUE
  ) %>%
  tokens_tolower() %>%
  tokens_remove(c( # Remove stopwords
    stopwords('en'),
    stopwords('es'),
    stopwords('nl'),
    stopwords('pt'),
    stopwords('ru'),
    stopwords('pt'),
    stopwords('de'),
    stopwords('fr'),
    stopwords('it')
  )) %>%
  tokens_keep(c('^[a-z]+-?[a-z]*$'), valuetype = 'regex') %>% # Keep only tokens with format [letters](-[letters])
  tokens_wordstem(language = 'en') %>% # Stem words according to English
  tokens_remove(c( # Remove redundant and overlooked tokens
    'oecd', 'water', 'et', 'al', 'x', 'pdf',
    'yes', 'abbrev', 'page', 'pp', 'p', 'er',
    'doi', 'can'
  ))

# Convert tokens to document frequency matrix
docs_dfm = docs_tokens %>%
  dfm() %>%
  dfm_trim(min_termfreq = 2)

docs_dfm

```

```{r show-top-tokens}

topfeatures(docs_dfm, n = 50)

```

```{r fit-stm-free, eval=params$fit}

# Fit STM and estimate number of topics (K = 0)
stm_fit_free = stm(
  docs_dfm,
  K = 0,
  data = docvars(docs_dfm),
  prevalence = ~ s(year) + s(finance) + type + region,
  seed = 2022,
  verbose = FALSE
)

save(stm_fit_free, file = 'stm_fit_free.RData')

```

```{r load-stm-free, eval=!params$fit}

load('stm_fit_free.RData')

```


```{r fun-stm-diag}

# Function for calculating STM diagnostics
compute_stm_diag = function(stm_obj, dfm_obj) {

  exp_topic_props = colMeans(make.dt(stm_obj)[,-1])
  
  topic_corr = topicCorr(stm_obj)
  
  diag(topic_corr$cor) = NA
  
  corr_topics = apply(topic_corr$cor, 2, max, na.rm = TRUE)
  
  cohe_topics = semanticCoherence(stm_obj, dfm_obj, M = 10)
  excl_topics = exclusivity(stm_obj, M = 10)
  
  topic_df = tibble(
    frequency = exp_topic_props,
    correlated = corr_topics,
    coherence = cohe_topics,
    exclusivity = excl_topics
  )
  
  return(topic_df)
}

```


```{r plot-stm-free-diag}

# Compute diagnostics for free STM
topic_df_free = compute_stm_diag(stm_fit_free, docs_dfm)

p = ggplot(topic_df_free, aes(x = coherence, y = exclusivity, color = correlated, alpha = frequency)) +
  geom_point()

ggMarginal(p, type = 'density')

```

```{r show-stm-free-corr}

# Show number of topics with correlation close to 1
sum(topic_df_free$correlated > 0.99)

```


```{r fit-stm-base, eval=params$fit}

# Fit STM with number of topics from free model minus the number of highly correlated topics
stm_fit_base = stm(
  docs_dfm,
  K = nrow(topic_df_free) - sum(topic_df_free$correlated > 0.99),
  data = docvars(docs_dfm),
  prevalence = ~ s(year) + s(finance) + type + region,
  seed = 2022,
  verbose = FALSE
)

save(stm_fit_base, file = 'stm_fit_base.RData')

```

```{r load-stm-base, eval=!params$fit}

load('stm_fit_base.RData')

```

```{r show-stm-base-diag}

# Compute diagnostics for base STM
topic_df_base = compute_stm_diag(stm_fit_base, docs_dfm)

p = ggplot(topic_df_base, aes(
  x = coherence,
  y = exclusivity,
  color = correlated,
  alpha = frequency
)) +
  geom_point()

ggMarginal(p, type = 'density')

```

```{r show-stm-base-corr}

# Show number of highly correlated topics in base STM
sum(topic_df_base$correlated > 0.99)

```

```{r search-base, eval=params$fit}

# Search topic space around number of topics from base model
docs_stm = asSTMCorpus(docs_dfm)

stm_search_from_base = searchK(
  docs_stm$documents,
  docs_stm$vocab,
  K = seq(nrow(topic_df_base) - 10, nrow(topic_df_base) + 10, 2),
  verbose = FALSE
)

save(stm_search_from_base, file = 'stm_search_from_base.RData')

```

```{r load-search-base, eval=!params$fit}

docs_stm = asSTMCorpus(docs_dfm)

load('stm_search_from_base.RData')

```

```{r plot-search-base-diag}

# Compare diagnostics for topic number search
search_df_base = stm_search_from_base$results %>% 
  pivot_longer(cols = -1, names_to = "metric")

ggplot(search_df_base, aes(x = as.numeric(K), y = as.numeric(value))) +
  facet_wrap(vars(metric), scales = 'free_y') +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = nrow(topic_df_base), linetype = 'dashed')

```

```{r set-final-k}

# Settle on a final number of topics based on diagnostics comparison
final_K = 30

```

```{r search-final, eval=FALSE}

stm_search_from_final = searchK(
  docs_stm$documents,
  docs_stm$vocab,
  K = 29:39,
  verbose = FALSE
)

save(stm_search_from_final, file = 'stm_search_from_final.RData')

```

```{r plot-search-final-diag, eval=FALSE}

search_df_final = stm_search_from_final$results %>%
  pivot_longer(cols = -1, names_to = "metric")

ggplot(search_df_final, aes(x = as.numeric(K), y = as.numeric(value))) +
  facet_wrap(vars(metric), scales = 'free_y') +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = final_K, linetype = 'dashed')

```

```{r fit-stm-final, eval=params$fit}

# Fit STM with final number of topics
stm_fit_final = stm(
  docs_dfm,
  K = final_K,
  data = docvars(docs_dfm),
  prevalence = ~ s(year) + s(finance) + type + region,
  seed = 2022,
  verbose = FALSE
)

save(stm_fit_final, file = 'stm_fit_final.RData')

```

```{r load-stm-final, eval=!params$fit}

load('stm_fit_final.RData')

```

```{r plot-stm-final-diag}

# Compute diagnostics for final STM
topic_df_final = compute_stm_diag(stm_fit_final, docs_dfm) %>%
  mutate(id = row_number())

p = ggplot(topic_df_final, aes(
  x = coherence,
  y = exclusivity,
  color = correlated,
  alpha = frequency,
  label = id
)) +
  geom_point() +
  geom_label_repel(max.overlaps = 20)

ggMarginal(p, type = 'density')

```

```{r show-stm-final-topic-labels}

# Show topic labels for top 10 topics according to frequency
top_topics_final = topic_df_final %>%
  arrange(desc(frequency), desc(coherence), desc(exclusivity)) %>%
  pull(id) %>%
  head(10)

labelTopics(stm_fit_final, topics = top_topics_final, n = 10)

```

```{r plot-stm-final-topic-freq}

# Plot topic frequency and top topic labels
topic_freq_df = topic_df_final %>%
  mutate(
    label = apply(
      labelTopics(stm_fit_final, n = 5)$prob, 1, paste, collapse = ', '
    ),
    id = as.character(id)
  ) %>%
  arrange(desc(frequency))

ggplot(topic_freq_df, aes(
  x = frequency,
  y = reorder(id, frequency),
  label = label
)) +
  geom_col(orientation = 'y') +
  geom_text(aes(x = frequency + 0.03)) +
  xlim(0, 0.125)

```

```{r save-stm-final-topic-labels}

# Save topic topic labels as txt and csv
topic_labels_final = labelTopics(stm_fit_final, n = 10)

topic_labels_final_df = left_join(
  topic_freq_df %>% select(id, frequency),
  as_tibble(
    lapply(
      topic_labels_final[1:4], function(x) apply(x, 1, paste, collapse = ', ')
    )
  ) %>%
    mutate(id = as.character(row_number())),
  by = 'id'
) %>%
  arrange(desc(frequency)) %>%
  pivot_longer(
    cols = c('prob', 'frex', 'lift', 'score'),
    names_to = 'metric'
  ) %>%
  select(-frequency)

topic_labels_final_df_txt = topic_labels_final_df %>%
  mutate(
    id = ifelse(row_number() %% 4 == 1, id, ''),
    label = NA
  )

cnt = 0

for (i in seq(4, nrow(topic_labels_final_df), 4)) {
  topic_labels_final_df_txt = topic_labels_final_df_txt %>%
    add_row(.after = i+cnt)
  cnt = cnt + 1
}

write.table(
  topic_labels_final_df_txt,
  file = 'stm_final_topic_labels.txt',
  sep = '\t',
  row.names = FALSE,
  col.names = FALSE,
  quote = FALSE,
  na = ''
)

write.csv(
  topic_labels_final_df,
  file = 'stm_final_topic_labels.csv',
  row.names = FALSE,
  quote = FALSE,
  na = ''
)

```


```{r plot-stm-final-topic-net}

# Plot topic correlations
topic_corr_final = topicCorr(stm_fit_final, cutoff = 0.1)

plot.topicCorr(topic_corr_final, vlabels = 1:nrow(topic_corr_final$cor))

```

```{r fit-stm-final-effects}

# Estimate covariate effects for final topic model
effects_final = estimateEffect(
  ~ s(year) + s(finance) + type + region,
  stm_fit_final,
  metadata = docvars(docs_dfm)
)

```

```{r plot-stm-final-effects-year}

# Plot effect of covariate year on topic prevalence
effects_final_year = get_effects(effects_final, 'year', type = 'continuous') %>%
  mutate(topic = reorder(factor(topic), desc(proportion)))

ggplot(effects_final_year, aes(
  x = value,
  y = proportion,
  ymin = lower,
  ymax = upper
)) +
  facet_wrap(vars(topic)) +
  geom_line() +
  geom_ribbon(alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = 'dashed')

```

```{r plot-stm-final-effects-finance}

# Plot effect of covariate finance on topic prevalence
effects_final_finance = get_effects(effects_final, 'finance', type = 'pointestimate') %>%
  mutate(topic = reorder(factor(topic), desc(proportion)))

ggplot(effects_final_finance, aes(
  x = value,
  y = proportion,
  ymin = lower,
  ymax = upper
)) +
  facet_wrap(vars(topic)) +
  geom_pointrange(position = position_dodge(0.5)) +
  geom_line(aes(x = as.numeric(value))) +
  geom_hline(yintercept = 0, linetype = 'dashed')

```

```{r plot-stm-final-effects-type}

# Plot effect of covariate type as difference on topic prevalence
effects_final_type = get_effects(
  effects_final, 
  'type',
  type = 'difference',
  cov_val1 = 'Conceptual Piece',
  cov_val2 = 'Case Study'
) %>%
  mutate(topic = reorder(factor(topic), desc(difference)))

ggplot(effects_final_type, aes(
  x = topic,
  y = difference,
  ymin = lower,
  ymax = upper
)) +
  geom_pointrange() +
  geom_hline(yintercept = 0, linetype = 'dashed')

```
