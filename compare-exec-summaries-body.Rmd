---
title: "Compare Word Frequencies in Executive Summaries and Text Bodies"
author: "Malte LÃ¼ken"
date: '2022-07-18'
output: html_document
params:
  json: ['scrape-corpus', 'studies_on_water_scraped']
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r load-libraries}

library(rjson)
library(dplyr)
library(stringr)
library(tibble)
library(tidyr)
library(ggplot2)
library(ggExtra)
library(ggrepel)
library(quanteda)

```

```{r load-docs}

# Function for excluding chapters
exclude_sections = function(doc, terms) {
  exclude = str_detect(tolower(names(doc)), pattern = paste(terms, collapse = '|'))
  
  return(doc[!exclude])
}

# Read json files with scraped text from pdfs
json_filenames = list.files(
  paste(params$json, collapse = .Platform$file.sep),
  full.names = TRUE
)

docs = lapply(json_filenames, function(filename) {fromJSON(file = filename)})

# Strings for excluding chapters
terms = c(
  'preface', 'foreword', 'acknowledg', 'executive', 'summary', 'table',
  'figure', 'box', 'abbreviation', 'acronym', 'glossary', 'bibliography', 
  'note', 'meta', 'further reading', 'page',  'key messages', 'annex', 
  'refere', 'background materials', 'statistics'
)

# Exclude irrelevant chapters
docs_relevant = lapply(docs, exclude_sections, terms = terms)

# Create list of chapters included in analysis
included_chapters = str_remove_all(
  sapply(
    lapply(docs_relevant, names), paste, collapse = '\n\t'
  ),
  pattern = '\r'
)

write.table(included_chapters, file = 'included_chapters.txt', quote = FALSE)

```

```{r load-exec-summaries}

select_exec_summaries = function(doc) {
  include = str_detect(tolower(names(doc)), pattern = paste('exec'))
  
  return(doc[include])
}

exec_summaries = lapply(docs, select_exec_summaries)

```

```{r create-corpus, include=FALSE}

collapse_doc = function(doc) {
  return(paste(unlist(doc), collapse = ' '))
}

# Create corpus object from text and metadata
docs_corpus = corpus(
  c(sapply(docs_relevant, collapse_doc),
    sapply(exec_summaries, collapse_doc)),
  docnames = c(sapply(docs, function(doc) doc$meta$title),
               sapply(docs, function(doc) paste0('exec - ', doc$meta$title))),
  docvars = data.frame(type = rep(c('body', 'exec'), each = 55))
)

summary(docs_corpus, n = 5)

```

```{r prep-dfm, cache=TRUE, include=FALSE}

# Create tokenized corpus
docs_tokens = docs_corpus %>%
  tokens(
    remove_punct = TRUE,
    remove_symbols = TRUE,
    remove_numbers = TRUE,
    remove_url = TRUE
  ) %>%
  tokens_tolower() %>%
  tokens_remove(c( # Remove stopwords
    stopwords('en'),
    stopwords('es'),
    stopwords('nl'),
    stopwords('pt'),
    stopwords('ru'),
    stopwords('pt'),
    stopwords('de'),
    stopwords('fr'),
    stopwords('it')
  )) %>%
  tokens_keep(c('^[a-z]+-?[a-z]*$'), valuetype = 'regex') %>% # Keep only tokens with format [letters](-[letters])
  tokens_wordstem(language = 'en') %>% # Stem words according to English
  tokens_remove(c( # Remove redundant and overlooked tokens
    'oecd', 'water', 'et', 'al', 'x', 'pdf',
    'yes', 'abbrev', 'page', 'pp', 'p', 'er',
    'doi', 'can'
  ))

# Convert tokens to document frequency matrix
docs_dfm = docs_tokens %>%
  dfm() %>%
  dfm_trim(min_termfreq = 2)

docs_dfm

```

```{r split-dfm}

dfm_body = docs_tokens %>%
  tokens_subset(type == "body") %>%
  dfm() %>%
  dfm_trim(min_termfreq = 2)

dfm_exec = docs_tokens %>% 
  tokens_subset(type == "exec") %>%
  dfm() %>%
  dfm_trim(min_termfreq = 2)
```

```{r calc-features}

calc_feature_df = function(dfm_obj) {
  features = data.frame(
    feature = featnames(dfm_obj),
    feat_freq = colSums(dfm_obj),
    feat_prop = colMeans(dfm_obj %>%
  dfm_weight(scheme = 'prop')),
    tf_idf = colSums(dfm_obj %>% dfm_tfidf(scheme_tf = 'prop'))
  )
  
  return(features)
}

features_body = calc_feature_df(dfm_body)
features_exec = calc_feature_df(dfm_exec)

```

In this document, we compare word frequencies between executive summaries and the main text of reports from the OECD corpus "Studies on Water". The corpus consists of 55 reports of which all but one include an executive summary. For the main text, we included all chapters of each report, but excluded forewords, prefaces, appendices, and background material sections.

Before the comparison, we applied several preprocessing steps to remove irrelevant and nuisance words from the text: We excluded stop words for nine languages (e.g., "and", "or", "we" in English) as well as all words that did not follow the pattern <LETTERS><DASH><LETTERS>, where the <DASH> ("-") and second string of letters ("abcd") are optional. Moreover, we excluded the words "oecd" and "water" from the analysis. Finally all remaining words, were reduced to English word stems ("stemmed").

After the preprocessing, we calculated the frequency and proportion of every word in the main text and the executive summaries (proportion = word frequency/total word count in text). The following table shows the top 20 most frequent words in the main text (feat_freq = word frequency; feat_prop = word proportion):

```{r show-token-freq-body}

top_freq = 20

features_body %>% arrange(desc(feat_freq)) %>%
  select(-tf_idf) %>%
  remove_rownames() %>%
  head(top_freq)

```

The next table shows the top 20 most frequent words in the executive summaries:

```{r show-token-freq-exec}

top_freq = 20

features_exec %>% arrange(desc(feat_freq)) %>%
  select(-tf_idf) %>%
  remove_rownames() %>%
  head(top_freq)

```

```{r combine-feature-dfs}

features_inter = inner_join(
  features_body, features_exec,
  by = 'feature', suffix = c('_body', '_exec')
) %>%
  mutate(feat_prop_ratio = feat_prop_body/feat_prop_exec,
         feat_prop_dist = (feat_prop_body-feat_prop_exec)^2,
         tf_idf_ratio = tf_idf_body/tf_idf_exec,
         tf_idf_dist = (tf_idf_body-tf_idf_exec)^2)

```

```{r fun-plot-ratio-hist}

compute_binwidth = function(x) 2 * IQR(x) / (length(x)^(1/3))

plot_ratio_hist = function(df, ratio) {
  ggplot(df %>% filter(!is.na(ratio)), aes_string(x = ratio)) +
    geom_histogram(binwidth = compute_binwidth) +
    geom_vline(xintercept = 1, linetype = 2, size = 1)
}

```

```{r plot-ratio-hist-feat-prop, include=FALSE}

plot_ratio_hist(features_inter, 'feat_prop_ratio')

```


```{r plot-ratio-hist-tf-idf, include=FALSE}

plot_ratio_hist(features_inter, 'tf_idf_ratio')

```

```{r fun plot-ratio-regression-line}

plot_ratio_regression = function(df, var1, var2, sort) {
  ggplot(df %>% arrange(desc(!!as.name(sort))), aes_string(x = var1, y = var2, label = 'feature')) +
    geom_point(color = 'steelblue') +
    geom_abline(slope = 1, intercept = 0, linetype = 2) +
    geom_text(size = 2, check_overlap = TRUE)
}

```

To compare word proportions between both parts, we choose a visual approach and plot them against each other in a scatter plot. We focus on word proportions because the executive summaries contain much less total words than the main text, that is, we interpret frequencies relatively to the size of the text. The plot shows the proportions of different words in the main text (x-axis) vs. the executive summaries (y-axis). For example, it can be seen that the word stem *polici* is relatively frequent in both but much more in the executive summaries. In general, words that are above the dashed line are relatively more frequent in the executive summaries and words in the upper right corner are more frequent in both parts.

```{r plot-ratio-reg-prop}

plot_ratio_regression(features_inter, 'feat_prop_body', 'feat_prop_exec', 'feat_prop_dist') +
  labs(x = 'Word proportion main text', y = 'Word proportion exec summaries')

```

**Commentary on Differences in Word Frequencies (Malte)**

The results show substantial differences in relative word frequencies (i.e., proportions) between executive summaries and the main text. In the two tables, we can see that financial/economic terms are more prominent in the executive summaries. For example, the word stem "financ" is not included in the top 20 of the main text, whereas it is at rank four in the summaries. Similarly, the top 20 of the summaries include terms such as "invest" and "econom" which are not part of the main text top 20. This difference is supported by the figure, showing that terms such as "financ" and "invest" are relatively more frequent in the summaries.

Another difference is the more frequent use of governance terms in the executive summaries: Word stems such as "manag", "govern", and "polici" appear to be relatively *much* more frequent compared to the main text.

```{r plot-ratio-reg-tf-idf, include=FALSE}

plot_ratio_regression(features_inter, 'tf_idf_body', 'tf_idf_exec', 'tf_idf_dist')

```


```{r fun-show-key-features}

show_key_features = function(df, ratio, dist, n = 20) {
  df_head = df %>%
    arrange(desc(!!as.name(ratio))) %>%
    head(n)
  
  df_tail = df %>%
    arrange(desc(!!as.name(ratio))) %>%
    tail(n)
  
  df_equal = df %>%
    arrange(desc(!!as.name(dist))) %>%
    tail(n)
  
  return(rbind(df_head, df_tail, df_equal))
}

```

```{r show-key-features-feat-prop, include=FALSE}

show_key_features(features_inter, 'feat_prop_ratio', 'feat_prop_dist')

```

```{r show-key-features-tf-idf, include=FALSE}

show_key_features(features_inter, 'tf_idf_ratio', 'tf_idf_dist')

```
